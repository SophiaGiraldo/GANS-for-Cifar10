# -*- coding: utf-8 -*-
"""gansCIFAR10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rO4uMUzxvwvTXN1IhXgyVbk3TJa1cX1O
"""

!pip install tensorboardX

! pip freeze

import torch
from torch import nn, optim
from torch.autograd.variable import Variable
from torchvision import transforms, datasets

## It is not necessary that you understand the code in this file, 
## as it is only used for visualization purposes.


import os
import numpy as np
import errno
import torchvision.utils as vutils
from tensorboardX import SummaryWriter
from IPython import display
from matplotlib import pyplot as plt
import torch

'''
    TensorBoard Data will be stored in './runs' path
'''


class Logger:

    def __init__(self, model_name, data_name):
        self.model_name = model_name
        self.data_name = data_name

        self.comment = '{}_{}'.format(model_name, data_name)
        self.data_subdir = '{}/{}'.format(model_name, data_name)

        # TensorBoard
        self.writer = SummaryWriter(comment=self.comment)

    def log(self, d_error, g_error, epoch, n_batch, num_batches):

        # var_class = torch.autograd.variable.Variable
        if isinstance(d_error, torch.autograd.Variable):
            d_error = d_error.data.cpu().numpy()
        if isinstance(g_error, torch.autograd.Variable):
            g_error = g_error.data.cpu().numpy()

        step = Logger._step(epoch, n_batch, num_batches)
        self.writer.add_scalar(
            '{}/D_error'.format(self.comment), d_error, step)
        self.writer.add_scalar(
            '{}/G_error'.format(self.comment), g_error, step)

    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):
        '''
        input images are expected in format (NCHW)
        '''
        if type(images) == np.ndarray:
            images = torch.from_numpy(images)
        
        if format=='NHWC':
            images = images.transpose(1,3)
        

        step = Logger._step(epoch, n_batch, num_batches)
        img_name = '{}/images{}'.format(self.comment, '')

        # Make horizontal grid from image tensor
        horizontal_grid = vutils.make_grid(
            images, normalize=normalize, scale_each=True)
        # Make vertical grid from image tensor
        nrows = int(np.sqrt(num_images))
        grid = vutils.make_grid(
            images, nrow=nrows, normalize=True, scale_each=True)

        # Add horizontal images to tensorboard
        self.writer.add_image(img_name, horizontal_grid, step)

        # Save plots
        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)

    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):
        out_dir = './data/images/{}'.format(self.data_subdir)
        Logger._make_dir(out_dir)

        # Plot and save horizontal
        fig = plt.figure(figsize=(16, 16))
        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))
        plt.axis('off')
        if plot_horizontal:
            display.display(plt.gcf())
        self._save_images(fig, epoch, n_batch, 'hori')
        plt.close()

        # Save squared
        fig = plt.figure()
        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))
        plt.axis('off')
        self._save_images(fig, epoch, n_batch)
        plt.close()

    def _save_images(self, fig, epoch, n_batch, comment=''):
        out_dir = './data/images/{}'.format(self.data_subdir)
        Logger._make_dir(out_dir)
        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,
                                                         comment, epoch, n_batch))

    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):
        
        # var_class = torch.autograd.variable.Variable
        if isinstance(d_error, torch.autograd.Variable):
            d_error = d_error.data.cpu().numpy()
        if isinstance(g_error, torch.autograd.Variable):
            g_error = g_error.data.cpu().numpy()
        if isinstance(d_pred_real, torch.autograd.Variable):
            d_pred_real = d_pred_real.data
        if isinstance(d_pred_fake, torch.autograd.Variable):
            d_pred_fake = d_pred_fake.data
        
        
        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(
            epoch,num_epochs, n_batch, num_batches)
             )
        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))
        print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))

    def save_models(self, generator, discriminator, epoch):
        out_dir = './data/models/{}'.format(self.data_subdir)
        Logger._make_dir(out_dir)
        torch.save(generator.state_dict(),
                   '{}/G_epoch_{}'.format(out_dir, epoch))
        torch.save(discriminator.state_dict(),
                   '{}/D_epoch_{}'.format(out_dir, epoch))

    def close(self):
        self.writer.close()

    # Private Functionality

    @staticmethod
    def _step(epoch, n_batch, num_batches):
        return epoch * num_batches + n_batch

    @staticmethod
    def _make_dir(directory):
        try:
            os.makedirs(directory)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise

def cifar10_data():
    
    compose = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    out_dir = './dataset'
    return datasets.CIFAR10(root = out_dir, train=True, transform=compose, download=True)
# Load data
data = cifar10_data()

# Create loader with data, so that we can iterate over it
data_loader = torch.utils.data.DataLoader(data, batch_size=300, shuffle=True)

# Num batches
num_batches = len(data_loader)



class DiscriminatorNet(torch.nn.Module): #nn.Module: base class for all neural network modules.
    """
    A three hidden-layer discriminative neural network
    """
    def __init__(self):
        super(DiscriminatorNet, self).__init__() #initializing the discriminator network (DN)
        n_features = 3072                        #input of DN
        n_out = 1                                #output of DN -> scalar (probability)
        
        self.hidden0 = nn.Sequential(            #first hidden layer 
            nn.Linear(n_features, 1024),         #applies a linear transformation to the incoming data -> bigger to smaller
            nn.LeakyReLU(0.2),                   #it allows a small gradient
            nn.Dropout(0.3)                      #regularization technique: prevents overfitting
                                                 #randomly selected neurons are ignored during training. 
                                                 #They are “dropped-out” randomly. This means that their 
                                                 #contribution to the activation of downstream neurons 
                                                 #is temporally removed on the forward pass and any weight 
                                                 #updates are not applied to the neuron on the backward pass.
        )
        self.hidden1 = nn.Sequential(            #second hidden layer
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.hidden2 = nn.Sequential(            #third hidden layer
            nn.Linear(512, 256),                 
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.out = nn.Sequential(                #output layer
            torch.nn.Linear(256, n_out),
            torch.nn.Sigmoid()                   #outputs a number between 0-1
        )

    def forward(self, x):
        x = self.hidden0(x)
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.out(x)
        return x

discriminator = DiscriminatorNet()              #activate DN -> It starts with an image and will make it smaller everytime
                                                #until it outputs a single number (probability)



def images_to_vectors(images):                  #transform Cifar10 images to vectors
    return images.view(images.size(0), 3072)

def vectors_to_images(vectors):                #transform vectors to images Cifar10
    return vectors.view(vectors.size(0), 3, 32, 32)

class GeneratorNet(torch.nn.Module):
    """
    A three hidden-layer generative neural network
    """
    def __init__(self):
        super(GeneratorNet, self).__init__()   #initialize the generator network
        n_features = 300                       #receives a sample noise with dimension 300
        n_out = 3072                           #outputs a vector with 3072 features (synthetic/fake images)
        
        self.hidden0 = nn.Sequential(          #first hidden layer
            nn.Linear(n_features, 456),        #from smaller to bigger
            nn.LeakyReLU(0.2)
        )
        self.hidden1 = nn.Sequential(          #second hidden layer
            nn.Linear(456, 612),
            nn.LeakyReLU(0.2)
        )
        self.hidden2 = nn.Sequential(          #third hidden layer
            nn.Linear(612, 2024),
            nn.LeakyReLU(0.2)
        )
        
        self.out = nn.Sequential(              #output layer: image
            nn.Linear(2024, n_out),
            nn.Tanh()
        )

    def forward(self, x):
        x = self.hidden0(x)
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.out(x)
        return x

generator = GeneratorNet()                     #activates GN -> begins with a sample of the noise dimension 
                                               # and outputs fake image with n_features dimension

def noise(size):  #generates noise 
    '''
    Generates a 1-d vector of gaussian sampled random values
    '''
    n = Variable(torch.randn(size, 300))
    return n

d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)  # Method for Stochastic Optimization
g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)

loss = nn.BCELoss()  #Creates a criterion that measures the Binary Cross Entropy between the target and the output
                     #targets 'y' should be numbers between 0 and 1.

def ones_target(size):
    '''
    Tensor containing ones, with shape = size
    '''
    data = Variable(torch.ones(size, 1))
    return data

def zeros_target(size):
    '''
    Tensor containing zeros, with shape = size
    '''
    data = Variable(torch.zeros(size, 1))
    return data

real_data.size(0)

def train_discriminator(optimizer, real_data, fake_data): #training discriminator with real and fake data
    N = real_data.size(0)   
    # Reset gradients
    optimizer.zero_grad()   #when you start your training loop, ideally you should zero out the gradients 
                            #so that you do the parameter update correctly. 
                            #Else the gradient would point in some other directions than the intended direction towards the minimum
    
    # 1.1 Train on Real Data
    prediction_real = discriminator(real_data) #probability of classifying a real image as real
    # Calculate error and backpropagate
    error_real = loss(prediction_real, ones_target(N) ) #error
    error_real.backward()                               #backpropagates the error

    # 1.2 Train on Fake Data
    prediction_fake = discriminator(fake_data) #probability of classifying a fake image as real
    
    # Calculate error and backpropagate
    error_fake = loss(prediction_fake, zeros_target(N))
    error_fake.backward()
    
    # 1.3 Update weights with gradients
    optimizer.step()   # performs a parameter update based on the current gradient 
                       #(stored in .grad attribute of a parameter) and the update rule. 
    
    # Return error and predictions for real and fake inputs
    return error_real + error_fake, prediction_real, prediction_fake

def train_generator(optimizer, fake_data):
    N = fake_data.size(0) 
    # Reset gradients
    optimizer.zero_grad()  #when you start your training loop, ideally you should zero out the gradients 
                            #so that you do the parameter update correctly. 
                            #Else the gradient would point in some other directions than the intended direction towards the minimum 

    # Sample noise and generate fake data
    prediction = discriminator(fake_data) #probability of classifying a fake image as real

    # Calculate error and backpropagate
    error = loss(prediction, ones_target(N)) 
    error.backward()

    # Update weights with gradients
    optimizer.step()

    # Return error
    return error

num_test_samples = 16
test_noise = noise(num_test_samples)

import matplotlib.pyplot as plt

# Create logger instance
logger = Logger(model_name='VGAN', data_name='CIFAR10')

# Total number of epochs to train
num_epochs = 10

for epoch in range(num_epochs):
    for n_batch, (real_batch,_) in enumerate(data_loader):
        N = real_batch.size(0)
        #N=300
        # 1. Train Discriminator
        real_data = Variable(images_to_vectors(real_batch)) #transform real data to vectors

        # Generate fake data and detach 
        # (so gradients are not calculated for generator)
        fake_data = generator(noise(N)).detach()            #fake data form the generator and noise

        # Train D
        d_error, d_pred_real, d_pred_fake = \
              train_discriminator(d_optimizer, real_data, fake_data)

        # 2. Train Generator

        # Generate fake data
        fake_data = generator(noise(N))

        # Train G
        g_error = train_generator(g_optimizer, fake_data)

        # Log batch error
        logger.log(d_error, g_error, epoch, n_batch, num_batches) #visualizing

        # Display Progress every few batches
        if (n_batch) % 167 == 0: 
            test_images = vectors_to_images(generator(test_noise))
            test_images = test_images.data

            logger.log_images(
                test_images, num_test_samples, 
                epoch, n_batch, num_batches
            );
            # Display status Logs
            logger.display_status(
                epoch, num_epochs, n_batch, num_batches,
                d_error, g_error, d_pred_real, d_pred_fake
            )
            
            #d_error_t += [d_error.data.cpu().numpy()] 
            #g_error_t += [g_error.data.cpu().numpy()]
            #dt += [d_error.item()]
            #gt += [g_error.item()]
            #t = [100,200,300,400,500,600]
            
            
            #plt.plot(d_error_t, t)
            #plt.show()
            
            
            #if (d_pred_real.mean() >= 0.5):
             # conf_mat[0][0] +=1
            #else:
             # conf_mat[0][1] +=1
           
            #if (d_pred_fake.mean() >= 0.4):
             # conf_mat[1][0] += 1
            #else:
             # conf_mat[1][1] += 1
              
              
              
            #print (conf_mat)

plt.plot(t, dt[:6], 'r--', t,dt[6:],'g--')
plt.axis([0, 800, 0.5, 1.4])
plt.show()

plt.plot(t, gt[:6], 'r--', t,gt[6:],'g--')
plt.axis([0, 800, 1, 2.7])
plt.show()

